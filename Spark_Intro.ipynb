{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are three main component of Spark \n",
    "\n",
    "### SparkContext --> communicate with cluster manager  ( as python object ) and create RDD's\n",
    "### SparkConf --> Define the configuration of spark to run\n",
    "### SparkShell --> To process and do data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with sparkContext you can pass Jobs, Schedules and Complete Tasks, Resources"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                 |---> Worker1\n",
    "sparkContext --> Spark Cluster --| \n",
    "                 Manager         |---> Worker2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc = SparkContext(\"local\",\"hads on Python\") # master and appName\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spark_documentation](https://spark.apache.org/docs/2.2.0/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3650, 1095, 12775, 9125, 14965, 3285, 10585]\n"
     ]
    }
   ],
   "source": [
    "# spark in execution \n",
    "visitors = [ 10,3,35,25,41,9,29]\n",
    "df_visitors = sc.parallelize(visitors) # createing RDD\n",
    "df_visitors_year = df_visitors.map(lambda x:x*365) # Transformation\n",
    "df_visitors_count = df_visitors_year.collect() #Action\n",
    "print(df_visitors_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spak Shell \n",
    "# pyspark shell operation df\n",
    "text_file = spark.read.text(\"/usr/local/spark/README.md\") # copied from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='# Apache Spark')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_with_spark = text_file.filter(text_file.value.contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read about SparkConf at it's official site\n",
    "#### many options like master, appName etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI Machine Learning Repository has Many DataSets to use for analysis and ML \n",
    "\n",
    "[uci_wesite](https://archive.ics.uci.edu/ml/index.php)\n",
    "#### search for kdd cup 1999 data set at this site\n",
    "[here is the link](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link to data http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\n",
    "\n",
    "import urllib.request # to fetch data from internet\n",
    "\n",
    "f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\",\"kddcup.data.gz\")\n",
    "#download and store file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so data is downloaded in current Folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkRDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./kddcup.data.gz\") #spark can read compress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./kddcup.data.gz MapPartitionsRDD[19] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization with Spark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(1,101))\n",
    "lst_rdd = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[20] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_rdd.count() #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_rdd.reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data in memory (very slow if we work with python object rather then parallize RDDs of pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_memory = raw_data.takeSample(False,10000,42) #replacement,nums,randomseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_normal_py = [ line.split(\",\") for line in data_in_memory \\\n",
    "                     if \"normal\" in line ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1981"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contain_normal_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting DataSets and Creating New Combinations with Set Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = raw_data.sample(False,.10,seed=42)\n",
    "normal_sample = sampled.filter(lambda line : \"normal\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97404"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_normal_sample = sampled.subtract(normal_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490705"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393301"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_normal_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490705"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sample.count() + non_normal_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = sampled.map(lambda line : line.split(\",\")).map(\n",
    "                                lambda f: f[1]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2 = sampled.map(lambda line : line.split(\",\")).map(\n",
    "                                lambda f: f[2]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = feature_1.collect()\n",
    "f2 = feature_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp', 'udp', 'icmp']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http', 'finger', 'auth', 'domain_u', 'smtp', 'ftp', 'telnet', 'eco_i', 'ntp_u', 'ecr_i', 'other', 'private', 'pop_3', 'ftp_data', 'daytime', 'remote_job', 'supdup', 'name', 'ssh', 'domain', 'gopher', 'time', 'rje', 'ctf', 'mtp', 'X11', 'urp_i', 'pm_dump', 'IRC', 'exec', 'bgp', 'nnsp', 'iso_tsap', 'http_443', 'login', 'shell', 'printer', 'efs', 'courier', 'uucp', 'kshell', 'klogin', 'whois', 'echo', 'discard', 'systat', 'netstat', 'hostnames', 'csnet_ns', 'pop_2', 'sunrpc', 'uucp_path', 'nntp', 'netbios_ns', 'netbios_ssn', 'netbios_dgm', 'imap4', 'sql_net', 'vmnet', 'link', 'Z39_50', 'ldap', 'urh_i', 'tftp_u', 'red_i', 'tim_i']\n"
     ]
    }
   ],
   "source": [
    "print(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cartesian all possible combinations ( product )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = feature_1.cartesian(feature_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tcp', 'http'),\n",
       " ('tcp', 'finger'),\n",
       " ('tcp', 'auth'),\n",
       " ('tcp', 'domain_u'),\n",
       " ('tcp', 'smtp'),\n",
       " ('tcp', 'ftp'),\n",
       " ('tcp', 'telnet'),\n",
       " ('tcp', 'eco_i'),\n",
       " ('tcp', 'ntp_u'),\n",
       " ('tcp', 'ecr_i'),\n",
       " ('tcp', 'other'),\n",
       " ('tcp', 'private'),\n",
       " ('tcp', 'pop_3'),\n",
       " ('tcp', 'ftp_data'),\n",
       " ('tcp', 'daytime'),\n",
       " ('tcp', 'remote_job'),\n",
       " ('tcp', 'supdup'),\n",
       " ('tcp', 'name'),\n",
       " ('tcp', 'ssh'),\n",
       " ('tcp', 'domain'),\n",
       " ('tcp', 'gopher'),\n",
       " ('tcp', 'time'),\n",
       " ('tcp', 'rje'),\n",
       " ('tcp', 'ctf'),\n",
       " ('tcp', 'mtp'),\n",
       " ('tcp', 'X11'),\n",
       " ('tcp', 'urp_i'),\n",
       " ('tcp', 'pm_dump'),\n",
       " ('tcp', 'IRC'),\n",
       " ('tcp', 'exec'),\n",
       " ('tcp', 'bgp'),\n",
       " ('tcp', 'nnsp'),\n",
       " ('tcp', 'iso_tsap'),\n",
       " ('tcp', 'http_443'),\n",
       " ('tcp', 'login'),\n",
       " ('tcp', 'shell'),\n",
       " ('tcp', 'printer'),\n",
       " ('tcp', 'efs'),\n",
       " ('tcp', 'courier'),\n",
       " ('tcp', 'uucp'),\n",
       " ('tcp', 'kshell'),\n",
       " ('tcp', 'klogin'),\n",
       " ('tcp', 'whois'),\n",
       " ('tcp', 'echo'),\n",
       " ('tcp', 'discard'),\n",
       " ('tcp', 'systat'),\n",
       " ('tcp', 'netstat'),\n",
       " ('tcp', 'hostnames'),\n",
       " ('tcp', 'csnet_ns'),\n",
       " ('tcp', 'pop_2'),\n",
       " ('tcp', 'sunrpc'),\n",
       " ('tcp', 'uucp_path'),\n",
       " ('tcp', 'nntp'),\n",
       " ('tcp', 'netbios_ns'),\n",
       " ('tcp', 'netbios_ssn'),\n",
       " ('tcp', 'netbios_dgm'),\n",
       " ('tcp', 'imap4'),\n",
       " ('tcp', 'sql_net'),\n",
       " ('tcp', 'vmnet'),\n",
       " ('tcp', 'link'),\n",
       " ('tcp', 'Z39_50'),\n",
       " ('tcp', 'ldap'),\n",
       " ('tcp', 'urh_i'),\n",
       " ('tcp', 'tftp_u'),\n",
       " ('tcp', 'red_i'),\n",
       " ('tcp', 'tim_i'),\n",
       " ('udp', 'http'),\n",
       " ('icmp', 'http'),\n",
       " ('udp', 'finger'),\n",
       " ('udp', 'auth'),\n",
       " ('icmp', 'finger'),\n",
       " ('icmp', 'auth'),\n",
       " ('udp', 'domain_u'),\n",
       " ('udp', 'smtp'),\n",
       " ('udp', 'ftp'),\n",
       " ('udp', 'telnet'),\n",
       " ('icmp', 'domain_u'),\n",
       " ('icmp', 'smtp'),\n",
       " ('icmp', 'ftp'),\n",
       " ('icmp', 'telnet'),\n",
       " ('udp', 'eco_i'),\n",
       " ('udp', 'ntp_u'),\n",
       " ('udp', 'ecr_i'),\n",
       " ('udp', 'other'),\n",
       " ('udp', 'private'),\n",
       " ('udp', 'pop_3'),\n",
       " ('udp', 'ftp_data'),\n",
       " ('udp', 'daytime'),\n",
       " ('icmp', 'eco_i'),\n",
       " ('icmp', 'ntp_u'),\n",
       " ('icmp', 'ecr_i'),\n",
       " ('icmp', 'other'),\n",
       " ('icmp', 'private'),\n",
       " ('icmp', 'pop_3'),\n",
       " ('icmp', 'ftp_data'),\n",
       " ('icmp', 'daytime'),\n",
       " ('udp', 'remote_job'),\n",
       " ('udp', 'supdup'),\n",
       " ('udp', 'name'),\n",
       " ('udp', 'ssh'),\n",
       " ('udp', 'domain'),\n",
       " ('udp', 'gopher'),\n",
       " ('udp', 'time'),\n",
       " ('udp', 'rje'),\n",
       " ('udp', 'ctf'),\n",
       " ('udp', 'mtp'),\n",
       " ('udp', 'X11'),\n",
       " ('udp', 'urp_i'),\n",
       " ('udp', 'pm_dump'),\n",
       " ('udp', 'IRC'),\n",
       " ('udp', 'exec'),\n",
       " ('udp', 'bgp'),\n",
       " ('icmp', 'remote_job'),\n",
       " ('icmp', 'supdup'),\n",
       " ('icmp', 'name'),\n",
       " ('icmp', 'ssh'),\n",
       " ('icmp', 'domain'),\n",
       " ('icmp', 'gopher'),\n",
       " ('icmp', 'time'),\n",
       " ('icmp', 'rje'),\n",
       " ('icmp', 'ctf'),\n",
       " ('icmp', 'mtp'),\n",
       " ('icmp', 'X11'),\n",
       " ('icmp', 'urp_i'),\n",
       " ('icmp', 'pm_dump'),\n",
       " ('icmp', 'IRC'),\n",
       " ('icmp', 'exec'),\n",
       " ('icmp', 'bgp'),\n",
       " ('udp', 'nnsp'),\n",
       " ('udp', 'iso_tsap'),\n",
       " ('udp', 'http_443'),\n",
       " ('udp', 'login'),\n",
       " ('udp', 'shell'),\n",
       " ('udp', 'printer'),\n",
       " ('udp', 'efs'),\n",
       " ('udp', 'courier'),\n",
       " ('udp', 'uucp'),\n",
       " ('udp', 'kshell'),\n",
       " ('udp', 'klogin'),\n",
       " ('udp', 'whois'),\n",
       " ('udp', 'echo'),\n",
       " ('udp', 'discard'),\n",
       " ('udp', 'systat'),\n",
       " ('udp', 'netstat'),\n",
       " ('udp', 'hostnames'),\n",
       " ('udp', 'csnet_ns'),\n",
       " ('udp', 'pop_2'),\n",
       " ('udp', 'sunrpc'),\n",
       " ('udp', 'uucp_path'),\n",
       " ('udp', 'nntp'),\n",
       " ('udp', 'netbios_ns'),\n",
       " ('udp', 'netbios_ssn'),\n",
       " ('udp', 'netbios_dgm'),\n",
       " ('udp', 'imap4'),\n",
       " ('udp', 'sql_net'),\n",
       " ('udp', 'vmnet'),\n",
       " ('udp', 'link'),\n",
       " ('udp', 'Z39_50'),\n",
       " ('udp', 'ldap'),\n",
       " ('udp', 'urh_i'),\n",
       " ('icmp', 'nnsp'),\n",
       " ('icmp', 'iso_tsap'),\n",
       " ('icmp', 'http_443'),\n",
       " ('icmp', 'login'),\n",
       " ('icmp', 'shell'),\n",
       " ('icmp', 'printer'),\n",
       " ('icmp', 'efs'),\n",
       " ('icmp', 'courier'),\n",
       " ('icmp', 'uucp'),\n",
       " ('icmp', 'kshell'),\n",
       " ('icmp', 'klogin'),\n",
       " ('icmp', 'whois'),\n",
       " ('icmp', 'echo'),\n",
       " ('icmp', 'discard'),\n",
       " ('icmp', 'systat'),\n",
       " ('icmp', 'netstat'),\n",
       " ('icmp', 'hostnames'),\n",
       " ('icmp', 'csnet_ns'),\n",
       " ('icmp', 'pop_2'),\n",
       " ('icmp', 'sunrpc'),\n",
       " ('icmp', 'uucp_path'),\n",
       " ('icmp', 'nntp'),\n",
       " ('icmp', 'netbios_ns'),\n",
       " ('icmp', 'netbios_ssn'),\n",
       " ('icmp', 'netbios_dgm'),\n",
       " ('icmp', 'imap4'),\n",
       " ('icmp', 'sql_net'),\n",
       " ('icmp', 'vmnet'),\n",
       " ('icmp', 'link'),\n",
       " ('icmp', 'Z39_50'),\n",
       " ('icmp', 'ldap'),\n",
       " ('icmp', 'urh_i'),\n",
       " ('udp', 'tftp_u'),\n",
       " ('udp', 'red_i'),\n",
       " ('udp', 'tim_i'),\n",
       " ('icmp', 'tftp_u'),\n",
       " ('icmp', 'red_i'),\n",
       " ('icmp', 'tim_i')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating and Summarizing Data into Useful Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating averages with map and reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211895753"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = raw_data.map(lambda x: x.split(\",\"))\n",
    "normal_data = csv.filter(lambda x : x[41] == \"normal.\")\n",
    "duration = normal_data.map(lambda x : int(x[0]))\n",
    "total_duration = duration.reduce(lambda x,y : x + y)\n",
    "total_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217.82472416710442"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_duration/(normal_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Average Computation with Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217.82472416710442"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate can calculate total_duration, count, all in one\n",
    "\n",
    "duration_count = normal_data.aggregate(\n",
    "    (0,0),\n",
    "    lambda db,new_value  : (db[0]+ int(new_value[0]),db[1]+1),\n",
    "    lambda db1,db2 : (db1[0]+db2[0],db1[1]+db2[1])\n",
    ")\n",
    "duration_count[0] / duration_count[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211895753, 972781)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tabling with Key Value Paired Data Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pivot Tables are used to Group Certain Values with Certain keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "* People having with different favorite fruits\n",
    "* How many people have \"Apple\" as their favorite fruit ? \n",
    "* Grouping based on : \n",
    "\n",
    "    * People -> value\n",
    "    * Fruit -> key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[64] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'tcp', 'http', 'SF', '215', '45076', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '0', '0', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', 'normal.']\n"
     ]
    }
   ],
   "source": [
    "print(csv.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 211895753.0),\n",
       " ('buffer_overflow.', 2751.0),\n",
       " ('loadmodule.', 326.0),\n",
       " ('perl.', 124.0),\n",
       " ('neptune.', 2.0),\n",
       " ('smurf.', 0.0),\n",
       " ('guess_passwd.', 144.0),\n",
       " ('pod.', 0.0),\n",
       " ('teardrop.', 0.0),\n",
       " ('portsweep.', 24257982.0),\n",
       " ('ipsweep.', 13049.0),\n",
       " ('land.', 0.0),\n",
       " ('ftp_write.', 259.0),\n",
       " ('back.', 284.0),\n",
       " ('imap.', 72.0),\n",
       " ('satan.', 500.0),\n",
       " ('phf.', 18.0),\n",
       " ('nmap.', 0.0),\n",
       " ('multihop.', 1288.0),\n",
       " ('warezmaster.', 301.0),\n",
       " ('warezclient.', 627563.0),\n",
       " ('spy.', 636.0),\n",
       " ('rootkit.', 1008.0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv = csv.map(lambda x : ( x[41], float(x[0]))).reduceByKey(lambda x,y: x+y)\n",
    "kv_duration = csv.map(lambda x : ( x[41], float(x[0]))).reduceByKey(lambda x,y: x+y)\n",
    "kv_duration.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'normal.': 1,\n",
       "             'buffer_overflow.': 1,\n",
       "             'loadmodule.': 1,\n",
       "             'perl.': 1,\n",
       "             'neptune.': 1,\n",
       "             'smurf.': 1,\n",
       "             'guess_passwd.': 1,\n",
       "             'pod.': 1,\n",
       "             'teardrop.': 1,\n",
       "             'portsweep.': 1,\n",
       "             'ipsweep.': 1,\n",
       "             'land.': 1,\n",
       "             'ftp_write.': 1,\n",
       "             'back.': 1,\n",
       "             'imap.': 1,\n",
       "             'satan.': 1,\n",
       "             'phf.': 1,\n",
       "             'nmap.': 1,\n",
       "             'multihop.': 1,\n",
       "             'warezmaster.': 1,\n",
       "             'warezclient.': 1,\n",
       "             'spy.': 1,\n",
       "             'rootkit.': 1})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('normal.', 211895753.0): 1,\n",
       "             ('buffer_overflow.', 2751.0): 1,\n",
       "             ('loadmodule.', 326.0): 1,\n",
       "             ('perl.', 124.0): 1,\n",
       "             ('neptune.', 2.0): 1,\n",
       "             ('smurf.', 0.0): 1,\n",
       "             ('guess_passwd.', 144.0): 1,\n",
       "             ('pod.', 0.0): 1,\n",
       "             ('teardrop.', 0.0): 1,\n",
       "             ('portsweep.', 24257982.0): 1,\n",
       "             ('ipsweep.', 13049.0): 1,\n",
       "             ('land.', 0.0): 1,\n",
       "             ('ftp_write.', 259.0): 1,\n",
       "             ('back.', 284.0): 1,\n",
       "             ('imap.', 72.0): 1,\n",
       "             ('satan.', 500.0): 1,\n",
       "             ('phf.', 18.0): 1,\n",
       "             ('nmap.', 0.0): 1,\n",
       "             ('multihop.', 1288.0): 1,\n",
       "             ('warezmaster.', 301.0): 1,\n",
       "             ('warezclient.', 627563.0): 1,\n",
       "             ('spy.', 636.0): 1,\n",
       "             ('rootkit.', 1008.0): 1})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Summary Statistics with MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### what are summary statistics ? \n",
    "##### How do we use MLlib to create summary statistics ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLlib\n",
    "\n",
    "* Machine learning library that comes with Spark\n",
    "* A recent development and to use Spark's data processing capabilites\n",
    "* Gives more seamless, deployable solutions \n",
    "[Documentation](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powerfull exploratory data analysis with MLlib\n",
    "###### Regression, another central task in machine learning is all about predicting numbers. In this section, we explore Spark's capabilities to perform regression tasks with models like linear regression and SVMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing summary statistics with MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = raw_data.map(lambda x:x.split(','))\n",
    "duration = csv.map(lambda x: [int(x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.stat._statistics.MultivariateStatisticalSummary at 0x7f75ad7f82b0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48.34243046])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898431"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([523206.01584972])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723.3298112546713"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "sqrt(summary.variance()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pearson and Spearman to Discover Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Pearson's correlation coefficient is the covariance of the two variables divided by the product of their starndard deviations. The form of the definition involves a \"product moment\", that is, the mean (the first moment about the origin) of the product of the mean-adjusted random variables; hence the modifier product-moment in the name.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p(x,y) = cov(x,y)/std(x)*std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.00890383,  0.30144701],\n",
       "       [ 0.00890383,  1.        , -0.19510495],\n",
       "       [ 0.30144701, -0.19510495,  1.        ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = csv.map(lambda x:[x[0],x[4],x[5]])\n",
    "Statistics.corr(metrics,method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 4.12205545e-02, 2.03915936e-02],\n",
       "       [4.12205545e-02, 1.00000000e+00, 2.39337570e-04],\n",
       "       [2.03915936e-02, 2.39337570e-04, 1.00000000e+00]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(metrics,method=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML and data science revovles around the understanding of : \n",
    "###### Statistics\n",
    "###### How data behaves ? \n",
    "###### How machine learning models are grounded ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is big Calculator \n",
    "# PySpark hides the complexity of the mathematics underneath it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your hypotheses on large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson's Chi Square Test\n",
    "\n",
    "* How likely are differences observed by chance\n",
    "* References the chi-squared distributions\n",
    "* It references to the chi Square distributions\n",
    "* For more information on Chi Square Distribution - [Click Here](https://en.wikipedia.org/wiki/Chi-squared_distribution)\n",
    "* This has three variations\n",
    "* One of these is -- Goodness of fit test\n",
    "* It is an observed dataset distributed differently than a theoretical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 4 \n",
      "statistic = 0.5852136752136753 \n",
      "pValue = 0.9646925263439344 \n",
      "No presumption against null hypothesis: observed follows the same distribution as expected..\n"
     ]
    }
   ],
   "source": [
    "visitors_freq = Vectors.dense(0.13,0.61,0.8,0.5,0.3)\n",
    "print(Statistics.chiSqTest(visitors_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Structure on Your Big Data with SparkSQL\n",
    "#### Taking away labels means that we are in unsupervised learning territory. Spark has great support for clustering and dimensionality reduction algorithms.\n",
    "* Manipulating DataFrame with SparkSQL schemas\n",
    "* Using the Spark DSL to build queries for structured data operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = raw_data.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = csv.map(lambda p: Row(duration=int(p[0]),protocol=p[1],\\\n",
    "               service=p[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql_context.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|   10217|\n",
      "|   11610|\n",
      "|   13724|\n",
      "|   10934|\n",
      "|   12026|\n",
      "|    5506|\n",
      "|   12381|\n",
      "|    9733|\n",
      "|   17932|\n",
      "|   40504|\n",
      "|   11565|\n",
      "|   12454|\n",
      "|    9473|\n",
      "|   12865|\n",
      "|   11288|\n",
      "|   10501|\n",
      "|   14479|\n",
      "|   10774|\n",
      "|   10007|\n",
      "|   12828|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_context.sql(\"SELECT duration FROM rdd WHERE protocol='tcp' AND duration > 2000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Spark DSL to build queries for structured data operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|   10217|\n",
      "|   11610|\n",
      "|   13724|\n",
      "|   10934|\n",
      "|   12026|\n",
      "|    5506|\n",
      "|   12381|\n",
      "|    9733|\n",
      "|   17932|\n",
      "|   40504|\n",
      "|   11565|\n",
      "|   12454|\n",
      "|    9473|\n",
      "|   12865|\n",
      "|   11288|\n",
      "|   10501|\n",
      "|   14479|\n",
      "|   10774|\n",
      "|   10007|\n",
      "|   12828|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"duration\").filter(df.duration>2000).filter(df.protocol=='tcp').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
